#summary Change Sensitivity Test Prioritization

|| [https://code.google.com/p/fittest/source/browse/#svn%2Ftrunk%2Fother-FITTEST-tools%2Fregression src] || [https://drive.google.com/folderview?id=0B1lIbYI9LkEJWFNfRlozY3ZIczg&usp=sharing&tid=0BwFaBuXA-b2vaXVEdU54ZV9zUEE download] ||


= Change Sensitivity Test Prioritization (CSTP) =

The FITTEST CSTP tool copes with the challenge that the services of which FI applications are composed evolve very quickly to meet technology and business changes.  Hence service integrators face frequently a critical decision either to update their service compositions to the new versions of the services they are exploiting, or to stay with the old ones, knowing that the old services might have issues, risks, and limited support. More often than not, the latter is the preferred choice since the regression testing activities that need to check the compliance of a new service with a system under test (SUT) are time consuming since there are too many tests. Consequently, test prioritization is crucial because the time budget for testing is often limited. The FITTEST Regression Testing tools (CSTP), aim at automatically prioritizing a set of test cases based on their sensitivity to external changes. The key idea is to give priority to the tests that can detect a high number of artificial changes. Mutation techniques tailored to the inter-service communication are used to generate changes in the messages exchanged between the SUT and the services. These changes are similar to unexpected contents in the service messages that could happen due to service evolution. The test cases that detect such changes have a higher chance of detecting real changes that matter in external services. Therefore, they are ranked higher. 

The measuring of change sensitivity is divided into six steps as can be seen by the following figure:

http://fittest.googlecode.com/svn/wiki/REGRESSION.jpg

(1) executing the SUT on the regression test suite; (2) monitoring the service requests and collecting the response messages; (3) generating mutated responses by means of mutation operators, based on service assumptions; (4) for each test case, running the SUT and comparing the behaviour of the SUT when receiving the original response and when the mutated ones are received. Then (5) the sensitivity (mutation) score is calculated and (6) test cases are ranked by mutation score.

As an example, service clients communicate with services via message passing: clients send requests
to services and receive responses (see Figure 2 below).

Let _s_ be the service under consideration, _snew_ be a candidate that can substitute _s_. _snew_ is the subject of audit testing. Let _TS_ be the set of available test cases that are selected as candidate for audit testing. These are the test cases whose executions result in the invocation of _s_. _TS_ is used in audit testing of _snew_ with respect to the composition under consideration.

In practice, execution of the full suite _TS_ might involve too much time or might require a big monetary budget, if only a limited number of invocations of _snew_ is available for testing purposes (i.e., invocations in test mode) or if the invocation of _snew_ requires payment. Hence, the service integrator has to minimize and prioritize the test cases from _TS_  that are actually run against _snew_ during audit testing. The goal is then to prioritize the test cases in such a way that issues, if any, are detected by an initial small subset of the ranked test cases.

CSTP determines the _change sensitivity_ of the test cases and uses this metrics to rank the test cases, from the most sensitive to service changes to the least sensitive one. Change sensitivity measures how sensitive a test case is to changes occurring to the service under consideration. The rationale underlying this approach is that  new versions of services may produce service responses that are still compliant with the service interface (WSDL), but violate some assumptions made (often implicitly) by the service integrator when building the service composition. Thus, test cases that are more sensitive to these changes are executed first.

http://fittest.googlecode.com/svn/wiki/imgs/cstp_fig1.png

Specifically, we have defined a set of new mutation operators and we apply them to the service responses to inject artificial changes. Our mutation operators are based on a survey of implicit assumptions commonly made on Web service responses and on manual inspection of those parts of the service API which is described only informally (usually by comments inside the WSDL document). After applying our mutations, we measure change sensitivity for each test case by comparing the outputs of each test case in two situations: with the original response, and with the mutated responses. A change is detected if the behaviour of the service composition differs when the original service is used as compared to the mutated response. Similarly to the mutation score, the change sensitivity score is the number of changes detected (mutants killed) over all changes.

The measuring of change sensitivity is divided into six steps 

   # executing the SUT on the regression test suite TS; 
   # monitoring the service requests and collecting the response messages; 
   # generating mutated responses by means of mutation operators, based on service assumptions; 
   # for each test case, running the SUT and comparing the behaviour of the SUT when receiving the original response and when the mutated ones are received. 
   # the sensitivity (mutation) score is calculated 
   # test cases are ranked by mutation score.